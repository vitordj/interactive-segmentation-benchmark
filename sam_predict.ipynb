{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment Anything experiment\n",
    "\n",
    "Recreating ritm_predict on SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "import cv2\n",
    "import numpy as np\n",
    "from modules.dataset import ImageAnnotations\n",
    "\n",
    "!git clone https://github.com/facebookresearch/segment-anything.git\n",
    "\n",
    "os.chdir('segment-anything') \n",
    "sys.path.insert(0, '..') # allow you to import modules from sam repo\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam_input_points_from_annotations(annotation:ImageAnnotations, bbox_id:int, include_aux=True, coord_bb=True):\n",
    "    \"\"\"Get input points and labels from an ImageAnnotations object.\"\"\"\n",
    "    bbox = next(box for box in annotation.box_annotations if box['id'] == annotation.box_annotations[bbox_id]['id'])\n",
    "    input_points = []\n",
    "    input_labels = []\n",
    "    for point_id in bbox['points_inside']:\n",
    "        point = next(p for p in annotation.point_annotations if p['id'] == point_id)\n",
    "        aux_points = []\n",
    "        if point['is_aux']:\n",
    "            aux_points.append(point)\n",
    "        else:\n",
    "            if coord_bb:\n",
    "                input_points.append([point['coord_bb'][1], point['coord_bb'][0]])\n",
    "            else:\n",
    "                input_points.append([point['coords'][1], point['coords'][0]])\n",
    "            input_labels.append(point['is_positive'])\n",
    "        if include_aux and aux_points:\n",
    "            for aux_point in aux_points:\n",
    "                if coord_bb:\n",
    "                    input_points.append([aux_point['coord_bb'][1], aux_point['coord_bb'][0]])\n",
    "                else:\n",
    "                    input_points.append([[aux_point['coords'][1], aux_point['coords'][0]]])\n",
    "                input_labels.append(point['is_positive'])\n",
    "    return np.array(input_points), np.array(input_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sam_prediction_point(sam_predictor, annotations:ImageAnnotations, jsons_folder='../datasets/supervisely_annotations/'\n",
    "                       , wgisd_folder='../datasets/wgisd_annotations/', include_aux=True, wgisd_n_points=3):\n",
    "    \"\"\"Get a prediction from a sam_predictor object.\"\"\"\n",
    "    annotations.load_image('../datasets/images/' + annotations.image_name)\n",
    "    print('../datasets/images/' + annotations.image_name)\n",
    "    \n",
    "    if annotations.image_name.startswith('DSC'):\n",
    "            annotations.read_supervisely(jsons_folder + annotations.image_name + '.json')\n",
    "    elif annotations.image_name.startswith('SYH') or annotations.image_name.startswith('CSV'):\n",
    "        sufx = f'-{wgisd_n_points}point' if wgisd_n_points==1 else f'-{wgisd_n_points}points'\n",
    "        annotations.read_wgisd(wgisd_folder + annotations.image_name.replace('.jpg', '.npz'), excel_sufx=sufx)\n",
    "    \n",
    "    masks_list, scores_list, logits_list = [], [], []\n",
    "    for id, bb in enumerate(annotations.box_annotations): # ):\n",
    "        print(bb)\n",
    "        image = annotations.bb_image_2_np(bb)\n",
    "        sam_predictor.set_image(image)\n",
    "        input_points, input_labels = sam_input_points_from_annotations(annotations, id, include_aux=include_aux)\n",
    "        masks, scores, logits = sam_predictor.predict(\n",
    "            point_coords=input_points,\n",
    "            point_labels=input_labels,\n",
    "            multimask_output=False)\n",
    "        masks_list.append(masks), scores_list.append(scores), logits_list.append(logits)\n",
    "    return masks_list, scores_list, logits_list\n",
    "\n",
    "def get_sam_prediction_bbox(sam_predictor, annotations:ImageAnnotations, jsons_folder='../datasets/supervisely_annotations/'\n",
    "                       , wgisd_folder='../datasets/wgisd_annotations/', bounding_boxes_only=True):\n",
    "    \"\"\"Get a prediction from a sam_predictor object.\"\"\"\n",
    "    annotations.load_image('../datasets/images/' + annotations.image_name)\n",
    "    print('../datasets/images/' + annotations.image_name)\n",
    "\n",
    "    if annotations.image_name.startswith('DSC'):\n",
    "        annotations.read_supervisely(jsons_folder + annotations.image_name + '.json')\n",
    "    elif annotations.image_name.startswith('SYH') or annotations.image_name.startswith('CSV'):\n",
    "        annotations.read_wgisd(wgisd_folder + annotations.image_name.replace('.jpg', '.npz'))\n",
    "\n",
    "    converted_boxes = [[x1, y1, x2, y2] for bb in annotations.box_annotations for [x1, y1], [x2, y2] in [bb['coords']]]\n",
    "    input_boxes = torch.tensor(converted_boxes, device=sam_predictor.device)\n",
    "    transformed_boxes = sam_predictor.transform.apply_boxes_torch(input_boxes, annotations.image.shape[:2])\n",
    "\n",
    "    if not bounding_boxes_only:\n",
    "        # input_points = [point['coord'] for point in annotations.point_annotations if not point['is_aux']]\n",
    "        converted_coords = [[[point['coord'][1], point['coord'][0]]] for point in annotations.point_annotations if not point['is_aux']]\n",
    "        input_coords = torch.tensor(converted_coords, device=sam_predictor.device)\n",
    "        transformed_coords = sam_predictor.transform.apply_coords_torch(input_coords, annotations.image.shape[:2])\n",
    "        input_labels = [point['is_positive'] for point in annotations.point_annotations if not point['is_aux']]\n",
    "        transformed_labels = torch.tensor(input_labels, device=sam_predictor.device)\n",
    "    else:\n",
    "        transformed_coords, transformed_labels = None, None\n",
    "    sam_predictor.set_image(annotations.image)\n",
    "    masks, scores, logits = sam_predictor.predict_torch(\n",
    "        point_coords=transformed_coords,\n",
    "        point_labels=transformed_labels,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False)\n",
    "\n",
    "    return masks, scores, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"weights/sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time to iterate\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def save_sam_predictions(imgs, output_folder, sam_predictor, prediction_name, include_aux=True,\n",
    "                                                    jsons_folder='../datasets/supervisely_annotations/',\n",
    "                                                        wgisd_folder='../datasets/wgisd_annotations/', wgisd_n_points=None, bounding_boxes=False, bounding_boxes_only=True):\n",
    "    \"\"\"Save predictions from a sam_predictor object.\n",
    "    Args:\n",
    "        imgs: list of image names\n",
    "        output_folder: folder to save the predictions\n",
    "        sam_predictor: sam_predictor object\n",
    "        prediction_name: name of the prediction\n",
    "        include_aux: include auxiliary points in the prediction\n",
    "        jsons_folder: folder with supervisely annotations\n",
    "        wgisd_folder: folder with wgisd annotations\n",
    "        wgisd_n_points: number of points to be used in wgisd annotations\n",
    "        bounding_boxes: use bounding boxes instead of points\n",
    "        bounding_boxes_only: if bounding_boxes param is set to True, set this arg to true for only use bounding boxes\"\"\"\n",
    "    for img in imgs:\n",
    "        anot = ImageAnnotations(img)\n",
    "        print ('Processing', anot.image_name)\n",
    "        if bounding_boxes:\n",
    "            masks, scores, logits = get_sam_prediction_bbox(sam_predictor, anot, jsons_folder, wgisd_folder, bounding_boxes_only)\n",
    "            mask_squeezed = masks.squeeze(1)\n",
    "            combined_mask = torch.max(mask_squeezed, dim=0)[0]\n",
    "            anot.image_pred[prediction_name] = combined_mask.cpu().numpy().astype(np.uint8)\n",
    "            dados = {'full': anot.image_pred[prediction_name],\n",
    "                     'bbs': mask_squeezed.cpu().numpy().astype(np.uint8)}\n",
    "        else:\n",
    "            masks, scores, logits = get_sam_prediction_point(sam_predictor, anot, include_aux=include_aux, wgisd_n_points=wgisd_n_points)\n",
    "            anot.reconstruct_prediction_mask(masks, prediction_name)\n",
    "            dados = {'full': (anot.image_pred[prediction_name]).astype(np.uint8),\n",
    "                     'bbs': {i:(pred[0,:,:]).astype(np.uint8) for i, pred in enumerate(masks)}\n",
    "            }\n",
    "        \n",
    "        np.savez_compressed(f'{output_folder}/{anot.image_name}_{prediction_name}.npz', **dados, allow_pickle=True)\n",
    "        print(f'{anot.image_name} annotation saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing DSC_0109.JPG\n",
      "../datasets/images/DSC_0109.JPG\n",
      "DSC_0109.JPG annotation saved.\n",
      "Processing DSC_0110.JPG\n",
      "../datasets/images/DSC_0110.JPG\n",
      "DSC_0110.JPG annotation saved.\n",
      "Processing DSC_0111.JPG\n",
      "../datasets/images/DSC_0111.JPG\n",
      "DSC_0111.JPG annotation saved.\n",
      "Processing DSC_0112.JPG\n",
      "../datasets/images/DSC_0112.JPG\n",
      "DSC_0112.JPG annotation saved.\n",
      "Processing DSC_0113.JPG\n",
      "../datasets/images/DSC_0113.JPG\n",
      "DSC_0113.JPG annotation saved.\n",
      "Processing DSC_0114.JPG\n",
      "../datasets/images/DSC_0114.JPG\n",
      "DSC_0114.JPG annotation saved.\n",
      "Processing DSC_0115.JPG\n",
      "../datasets/images/DSC_0115.JPG\n",
      "DSC_0115.JPG annotation saved.\n",
      "Processing DSC_0120.JPG\n",
      "../datasets/images/DSC_0120.JPG\n",
      "DSC_0120.JPG annotation saved.\n",
      "Processing DSC_0122.JPG\n",
      "../datasets/images/DSC_0122.JPG\n",
      "DSC_0122.JPG annotation saved.\n",
      "Processing DSC_0124.JPG\n",
      "../datasets/images/DSC_0124.JPG\n",
      "DSC_0124.JPG annotation saved.\n",
      "Processing DSC_0126.JPG\n",
      "../datasets/images/DSC_0126.JPG\n",
      "DSC_0126.JPG annotation saved.\n",
      "Processing DSC_0129.JPG\n",
      "../datasets/images/DSC_0129.JPG\n",
      "DSC_0129.JPG annotation saved.\n",
      "Processing DSC_0142.JPG\n",
      "../datasets/images/DSC_0142.JPG\n",
      "DSC_0142.JPG annotation saved.\n",
      "Processing DSC_0144.JPG\n",
      "../datasets/images/DSC_0144.JPG\n",
      "DSC_0144.JPG annotation saved.\n",
      "Processing DSC_0150.JPG\n",
      "../datasets/images/DSC_0150.JPG\n",
      "DSC_0150.JPG annotation saved.\n",
      "Processing DSC_0152.JPG\n",
      "../datasets/images/DSC_0152.JPG\n",
      "DSC_0152.JPG annotation saved.\n",
      "Processing DSC_0313.JPG\n",
      "../datasets/images/DSC_0313.JPG\n",
      "DSC_0313.JPG annotation saved.\n",
      "Processing DSC_0315.JPG\n",
      "../datasets/images/DSC_0315.JPG\n",
      "DSC_0315.JPG annotation saved.\n",
      "Processing DSC_0316.JPG\n",
      "../datasets/images/DSC_0316.JPG\n",
      "DSC_0316.JPG annotation saved.\n",
      "Processing DSC_0317.JPG\n",
      "../datasets/images/DSC_0317.JPG\n",
      "DSC_0317.JPG annotation saved.\n",
      "Processing DSC_0318.JPG\n",
      "../datasets/images/DSC_0318.JPG\n",
      "DSC_0318.JPG annotation saved.\n",
      "Processing DSC_0319.JPG\n",
      "../datasets/images/DSC_0319.JPG\n",
      "DSC_0319.JPG annotation saved.\n",
      "Processing DSC_0320.JPG\n",
      "../datasets/images/DSC_0320.JPG\n",
      "DSC_0320.JPG annotation saved.\n",
      "Processing DSC_0321.JPG\n",
      "../datasets/images/DSC_0321.JPG\n",
      "DSC_0321.JPG annotation saved.\n",
      "Processing DSC_0324.JPG\n",
      "../datasets/images/DSC_0324.JPG\n",
      "DSC_0324.JPG annotation saved.\n",
      "Processing DSC_0325.JPG\n",
      "../datasets/images/DSC_0325.JPG\n",
      "DSC_0325.JPG annotation saved.\n",
      "Processing DSC_0328.JPG\n",
      "../datasets/images/DSC_0328.JPG\n",
      "DSC_0328.JPG annotation saved.\n",
      "Processing DSC_0329.JPG\n",
      "../datasets/images/DSC_0329.JPG\n",
      "DSC_0329.JPG annotation saved.\n",
      "Processing DSC_0331.JPG\n",
      "../datasets/images/DSC_0331.JPG\n",
      "DSC_0331.JPG annotation saved.\n",
      "Processing DSC_0332.JPG\n",
      "../datasets/images/DSC_0332.JPG\n",
      "DSC_0332.JPG annotation saved.\n",
      "Processing DSC_0340.JPG\n",
      "../datasets/images/DSC_0340.JPG\n",
      "DSC_0340.JPG annotation saved.\n",
      "Processing DSC_0342.JPG\n",
      "../datasets/images/DSC_0342.JPG\n",
      "DSC_0342.JPG annotation saved.\n",
      "Tempo bbs: 1669.1492717266083\n"
     ]
    }
   ],
   "source": [
    "from copy import copy\n",
    "all_jpgs = os.listdir('../datasets/images/')\n",
    "wgisd_csv = [img for img in all_jpgs if img.startswith('CSV')]\n",
    "wgisd_syh = [img for img in all_jpgs if img.startswith('SYH')]\n",
    "supervisely = [img for img in all_jpgs if img.startswith('DSC')]\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "# save_sam_predictions(wgisd_csv, '../datasets/sam_predictions/', predictor, f'sam_vit_h_3_points', include_aux=True, wgisd_n_points=3)\n",
    "# print('Tempo 3 pontos:', time.time() - start)\n",
    "\n",
    "\n",
    "# save_sam_predictions(supervisely, '../datasets/sam_predictions/', predictor, f'sam_vit_h_bbs', bounding_boxes=True)\n",
    "# print('Tempo bbs:', time.time() - start)\n",
    "\n",
    "for i in range(len(supervisely)):\n",
    "    predictor1 = copy(predictor)\n",
    "    save_sam_predictions(supervisely[i:i+1], '../datasets/sam_predictions/', predictor1, f'sam_vit_h_bbs_point', bounding_boxes=True,  bounding_boxes_only=False)\n",
    "# save_sam_predictions(supervisely, '../datasets/sam_predictions/', predictor, f'sam_vit_h_bbs_point', bounding_boxes=True, bounding_boxes_only=False)\n",
    "print('Tempo bbs e um ponto:', time.time() - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SYH_2017-04-27_1233.jpg\n",
      "../datasets/images/SYH_2017-04-27_1233.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1233.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1236.jpg\n",
      "../datasets/images/SYH_2017-04-27_1236.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1236.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1238.jpg\n",
      "../datasets/images/SYH_2017-04-27_1238.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1238.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1239.jpg\n",
      "../datasets/images/SYH_2017-04-27_1239.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1239.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1241.jpg\n",
      "../datasets/images/SYH_2017-04-27_1241.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1241.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1251.jpg\n",
      "../datasets/images/SYH_2017-04-27_1251.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1251.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1253.jpg\n",
      "../datasets/images/SYH_2017-04-27_1253.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1253.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1260.jpg\n",
      "../datasets/images/SYH_2017-04-27_1260.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1260.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1268.jpg\n",
      "../datasets/images/SYH_2017-04-27_1268.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1268.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1269.jpg\n",
      "../datasets/images/SYH_2017-04-27_1269.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1269.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1271.jpg\n",
      "../datasets/images/SYH_2017-04-27_1271.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1271.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1280.jpg\n",
      "../datasets/images/SYH_2017-04-27_1280.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1280.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1294.jpg\n",
      "../datasets/images/SYH_2017-04-27_1294.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1294.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1300.jpg\n",
      "../datasets/images/SYH_2017-04-27_1300.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1300.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1304.jpg\n",
      "../datasets/images/SYH_2017-04-27_1304.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1304.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1310.jpg\n",
      "../datasets/images/SYH_2017-04-27_1310.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1310.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1311.jpg\n",
      "../datasets/images/SYH_2017-04-27_1311.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1311.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1312.jpg\n",
      "../datasets/images/SYH_2017-04-27_1312.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1312.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1316.jpg\n",
      "../datasets/images/SYH_2017-04-27_1316.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1316.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1318.jpg\n",
      "../datasets/images/SYH_2017-04-27_1318.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1318.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1322.jpg\n",
      "../datasets/images/SYH_2017-04-27_1322.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1322.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1332.jpg\n",
      "../datasets/images/SYH_2017-04-27_1332.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1332.jpg annotation saved.\n",
      "Processing SYH_2017-04-27_1336.jpg\n",
      "../datasets/images/SYH_2017-04-27_1336.jpg\n",
      "WGisd annotations loaded.\n",
      "SYH_2017-04-27_1336.jpg annotation saved.\n",
      "Tempo bbs: 5670.655803203583\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(wgisd_syh)):\n",
    "    predictor1 = copy(predictor)\n",
    "    save_sam_predictions(wgisd_syh[i:i+1], '../datasets/sam_predictions/', predictor1, f'sam_vit_h_bbs_point', bounding_boxes=True,  bounding_boxes_only=False)\n",
    "# save_sam_predictions(supervisely[:1], '../datasets/sam_predictions/', predictor, f'sam_vit_h_bbs_point', bounding_boxes=True, bounding_boxes_only=False)\n",
    "\n",
    "print('Tempo bbs:', time.time() - start) # até o 16 -> 26,95min + 24,72min = 51,67min"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
